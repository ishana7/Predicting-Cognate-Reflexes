{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWyK8LZyfMCC"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade setuptools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mi5k-Btqjq4"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow=='2.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "AEteXjRI2574",
        "outputId": "5b70f0ba-fb44-4e81-a8ab-4debab8a339d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "  Using cached dgl-0.9.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from dgl) (1.21.5)\n",
            "Requirement already satisfied: tqdm in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from dgl) (4.64.0)\n",
            "Requirement already satisfied: networkx>=2.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from dgl) (2.7.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\kavya\\appdata\\roaming\\python\\python39\\site-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (1.26.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tqdm->dgl) (0.4.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.9.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exCg9pJCbWMO",
        "outputId": "add11ac3-87ca-4a89-99ef-031ec2c2db40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-gcn in c:\\users\\kavya\\anaconda3\\lib\\site-packages (0.15.0)Note: you may need to restart the kernel to use updated packages.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install keras-gcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_khVaichlvt4"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas\n",
        "# !pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement absl (from versions: none)\n",
            "ERROR: No matching distribution found for absl\n"
          ]
        }
      ],
      "source": [
        "%pip install absl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py in c:\\users\\kavya\\anaconda3\\lib\\site-packages (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install absl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\kavya\\anaconda3\\lib\\site-packages (2.11.0)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
            "Requirement already satisfied: packaging in c:\\users\\kavya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kavya\\appdata\\roaming\\python\\python39\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed keras-2.11.0\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\kavya\\anaconda3\\lib\\site-packages (22.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow in c:\\users\\kavya\\anaconda3\\lib\\site-packages (2.11.0)\n",
            "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\kavya\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kavya\\appdata\\roaming\\python\\python39\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip\n",
        "# %pip uninstall tensorflow\n",
        "%pip install tensorflow\n",
        "# import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N90cMy8EYw6N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Sequence\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras_gcn import GraphConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kLWlTZ5gY9nu"
      },
      "outputs": [],
      "source": [
        "def build_base_data_and_vocab():\n",
        "  \"\"\"Builds vocabulary from the training data files.\"\"\"\n",
        "  #file= 'data_dir/'+file\n",
        "  vocab = set()\n",
        "  cognates = []\n",
        "  filepath = 'data_dir/training-mod-0.10.tsv'\n",
        "  print('Preparing base training data from %s ...', filepath)\n",
        "  with open(filepath, 'r', encoding='utf-8') as f:\n",
        "    # Skip header.\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "      for p in parts:\n",
        "        for c in p.split():\n",
        "          vocab.add(c)\n",
        "      cognates.append([p.strip() for p in parts])\n",
        "      \n",
        "  vocab = ['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>'] + sorted(\n",
        "      list(vocab))\n",
        "  vocab_size = len(vocab)\n",
        "  embedding_dim = 16\n",
        "  # tf_embed = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "  # tf.convert_to_tensor(cognates)\n",
        "  # embeddings= tf_embed(cognates)\n",
        "  # print(\"generated embedding:\", embeddings[0])\n",
        "  # print(\"COGSETS:\",cogsets,\"**********************\",\"\\n\")\n",
        "  # print(\"VOCAB:\",vocab,\"$$$$$$$$$$$$$$$$$$$$$$$\",\"\\n\")\n",
        "  return cognates, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nMfM0EGRZUzP"
      },
      "outputs": [],
      "source": [
        "def get_hparams():\n",
        "  \"\"\"Builds hyper-parameter dictionary from flags or file.\"\"\"\n",
        "  with open('checkpoint_dir/hparams.json', 'r') as f:\n",
        "    hparams = json.load(f)\n",
        "    print('HParams: %s', hparams)\n",
        "    # print(\"HPARAMS\",hparams,\"\\n\")\n",
        "    return hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zk7nibQeaDqc"
      },
      "outputs": [],
      "source": [
        "def expand_training_set(cogsets):\n",
        "  \"\"\"Expands the dataset to all possible variations.\"\"\"\n",
        "  print('Expanding training data ...')\n",
        "  nlangs = len(cogsets[0])\n",
        "  all_samples = []\n",
        "  for cs in cogsets:\n",
        "    # Find all valid positions.\n",
        "    isample = []\n",
        "    for i in range(nlangs):\n",
        "      if cs[i]:\n",
        "        isample.append(cs[i])\n",
        "      else:\n",
        "        isample.append('<BLANK>')\n",
        "    all_samples.append(isample)\n",
        "  random.shuffle(all_samples)\n",
        "  # print(\"ALL SAMPLES:\",all_samples,\"\\n\")\n",
        "  return all_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O-YTgUDhYxcS"
      },
      "outputs": [],
      "source": [
        "def build_train_dataset(all_samples, batch_size, nlangs, max_length, char2idx):\n",
        "  \"\"\"Creates train dataset from the generator.\"\"\"\n",
        "  # Create data generators to feed into the networks.\n",
        "  def la_gen():\n",
        "    while True:\n",
        "      for icset in all_samples:\n",
        "        inputs = []\n",
        "        targets = []\n",
        "        input_mask = []\n",
        "        target_mask = []\n",
        "        # Get the present items.\n",
        "        valids = [i for i in range(len(icset)) if icset[i] != '<BLANK>']\n",
        "        # Select how many inputs will be present to provide information.\n",
        "        num_present = random.randint(1, len(valids))\n",
        "        present = random.sample(valids, num_present)\n",
        "        # Create the actual data content.\n",
        "        for i in range(len(icset)):\n",
        "          # Create a max_length sequence.\n",
        "          template = [char2idx['<BLANK>']] * max_length\n",
        "          seq = [char2idx['<BOS>']] + [\n",
        "              char2idx[c] if c in char2idx else char2idx['<UNK>']\n",
        "              for c in icset[i].split()\n",
        "          ] + [char2idx['<EOS>']]\n",
        "          for j in range(min(len(seq), max_length)):\n",
        "            template[j] = seq[j]\n",
        "          targets.append(template)\n",
        "          inputs.append(template)\n",
        "          # If the sequence if valid, get gradient from it.\n",
        "          if i in valids:\n",
        "            target_mask.append([1.0] * max_length)\n",
        "          else:\n",
        "            target_mask.append([0.0] * max_length)\n",
        "          # If the sequence should be present, don't mask it.\n",
        "          if i in present:\n",
        "            input_mask.append([1.0] * max_length)\n",
        "          else:\n",
        "            input_mask.append([0.0] * max_length)\n",
        "        # Convert to required tensor formats.\n",
        "        inputs = tf.constant(inputs, dtype='int32')\n",
        "        targets = tf.constant(targets, dtype='int32')\n",
        "        input_mask = tf.constant(input_mask, dtype='float32')\n",
        "        target_mask = tf.constant(target_mask, dtype='float32')\n",
        "\n",
        "        yield (inputs, targets, input_mask, target_mask)\n",
        "\n",
        "  return tf.data.Dataset.from_generator(\n",
        "      la_gen,\n",
        "      output_signature=(tf.TensorSpec(\n",
        "          shape=(nlangs, max_length), dtype='int32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(nlangs, max_length),\n",
        "                            dtype='int32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=( nlangs, max_length),\n",
        "                            dtype='float32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=( nlangs, max_length),\n",
        "                            dtype='float32')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DY-LLJt6a2Zi"
      },
      "outputs": [],
      "source": [
        "# from keras_dgl.layers import GraphCNN\n",
        "# from keras.layers import Dense, Activation, Dropout\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.regularizers import l2\n",
        "# from keras.optimizers import Adam\n",
        "# import keras.backend as K\n",
        "# import numpy as np\n",
        "# from numpy import shape\n",
        "\n",
        "# from keras_dgl.utils import *\n",
        "# from keras_dgl.layers import GraphCNN\n",
        "# import keras\n",
        "# import numpy as np\n",
        "# from numpy.linalg import norm\n",
        "# class Infiller(tf.keras.Model):\n",
        "#   \"\"\"The infiller convolutional model.\"\"\"\n",
        "\n",
        "#   def __init__(self, vocab_size, hparams, batch_size, nlangs, max_length):\n",
        "#     super(Infiller, self).__init__()\n",
        "#     best_error = None\n",
        "\n",
        "#     SYM_NORM = True\n",
        "#     vocab_size = vocab_size\n",
        "#     embedding_dim = hparams['embedding_dim']\n",
        "#     embedding = np.random.randint(500, size=(nlangs, vocab_size))\n",
        "#     arr = []\n",
        "#     for i in range(len(embedding)):\n",
        "#         c = 0\n",
        "#         a = []\n",
        "#         for j in range(len(embedding)):\n",
        "#             cosine = np.dot(embedding[i], embedding[j]) / (norm(embedding[i]) * norm(embedding[j]))\n",
        "#             a.append(cosine)\n",
        "#         arr.append(a)\n",
        "#     A_norm = preprocess_adj_numpy(arr, SYM_NORM)\n",
        "#     num_filters = 2\n",
        "#     graph_conv_filters = np.concatenate([A_norm, np.matmul(A_norm, A_norm)], axis=0)\n",
        "#     graph_conv_filters = K.constant(graph_conv_filters)\n",
        "#     #   print(shape(graph_conv_filters))\n",
        "\n",
        "#     model = Sequential()\n",
        "\n",
        "    \n",
        "#     model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "#     model.add(Activation('softmax'))\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
        "\n",
        "#   def call(self, test):\n",
        "#     # Reshape the mask.\n",
        "    \n",
        "#     self.model.fit(inputs, targs, batch_size=input_i.shape[0], epochs=1, shuffle=False, verbose=0)\n",
        "\n",
        "#     # Embed the inputs.\n",
        "#     inputs = self.embedding(inputs)\n",
        "#     inputs = inputs * rmask\n",
        "\n",
        "#     # Scale the inputs.\n",
        "#     sfactor = (self.nlangs * self.max_length) / tf.math.reduce_sum(input_mask)\n",
        "#     if self.scale_pos == 'inputs':\n",
        "#       inputs = inputs * sfactor\n",
        "\n",
        "#     # Convolve\n",
        "#     inputs = self.conv(inputs)\n",
        "#     if self.scale_pos == 'conv':\n",
        "#       inputs = inputs * sfactor\n",
        "#     inputs = self.dropout(inputs, training=training)\n",
        "#     inputs = self.act(inputs)\n",
        "\n",
        "#     # Deconvolve\n",
        "#     logits = self.deconv(inputs)\n",
        "\n",
        "#     return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xeKTPxOJbRB_"
      },
      "outputs": [],
      "source": [
        "# @tf.function\n",
        "# def loss_function(real, pred, mask):\n",
        "#   # real shape = (BATCH_SIZE, max_length_output)\n",
        "#   # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "#   cross_entropy = tf.keras.losses.CategoricalCrossentropy(\n",
        "#       from_logits=True, reduction='none')\n",
        "#   loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "#   loss = mask * loss\n",
        "#   loss = tf.reduce_sum(loss)\n",
        "#   return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vvo-9v-ibPhu"
      },
      "outputs": [],
      "source": [
        "# @tf.function\n",
        "# def train_step(infiller, optimizer, inp, inp_mask, targ, targ_mask):\n",
        "#   \"\"\"Single training step.\"\"\"\n",
        "#   loss = 0\n",
        "#   with tf.GradientTape() as tape:\n",
        "#     logits = infiller(inp, inp_mask, training=True)\n",
        "#     loss = loss_function(targ, logits, targ_mask)\n",
        "#   variables = infiller.trainable_variables\n",
        "#   gradients = tape.gradient(loss, variables)\n",
        "#   optimizer.apply_gradients(zip(gradients, variables))\n",
        "#   return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UiECAcY_bcci"
      },
      "outputs": [],
      "source": [
        "from numpy import shape\n",
        "def evaluate_cset(infiller, cset, char2idx, max_length):\n",
        "  \"\"\"Evaluates given cognate set.\"\"\"\n",
        "  tgt_index = 0\n",
        "  inputs = []\n",
        "  input_mask = []\n",
        "  # Find possible target positions\n",
        "  for i, p in enumerate(cset):\n",
        "    if p.strip():\n",
        "      if p == '<TARGET>':\n",
        "        tgt_index = i\n",
        "        inputs.append([char2idx['<TARGET>']] * max_length)\n",
        "        input_mask.append([0.0] * max_length)\n",
        "      else:\n",
        "        seq = [char2idx['<BOS>']] + [\n",
        "            char2idx[c] if c in char2idx else char2idx['<UNK>']\n",
        "            for c in p.split()\n",
        "        ] + [char2idx['<EOS>']]\n",
        "        template = [char2idx['<BLANK>']] * max_length\n",
        "        for j in range(min(len(seq), max_length)):\n",
        "          template[j] = seq[j]\n",
        "        inputs.append(template)\n",
        "        input_mask.append([1.0] * max_length)\n",
        "    else:\n",
        "      inputs.append([char2idx['<BLANK>']] * max_length)\n",
        "      input_mask.append([0.0] * max_length)\n",
        "\n",
        "  inputs = tf.constant([inputs], dtype='int32')\n",
        "  input_mask = tf.constant([input_mask], dtype='float32')\n",
        "\n",
        "  logits = model.predict(input_mask, batch_size=inp_mask.shape[1])\n",
        "  trow = tf.math.argmax(logits[tgt_index,:], axis=-1)\n",
        "  return trow.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1WISb-SrvEt",
        "outputId": "6cb6ab86-9d3e-4db1-b80a-22f9527e3927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in c:\\users\\kavya\\anaconda3\\lib\\site-packages (1.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_adj(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
        "    else:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj).tocsr()\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def normalize_adj_numpy(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d)\n",
        "    else:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj)\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, symmetric=True):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_numpy(adj, symmetric=True):\n",
        "    adj = adj + np.eye(adj.shape[0])\n",
        "    adj = normalize_adj_numpy(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
        "    adj_out_tensor = []\n",
        "    for i in range(adj_tensor.shape[0]):\n",
        "        adj = adj_tensor[i]\n",
        "        adj = adj + np.eye(adj.shape[0])\n",
        "        adj = normalize_adj_numpy(adj, symmetric)\n",
        "        adj_out_tensor.append(adj)\n",
        "    adj_out_tensor = np.array(adj_out_tensor)\n",
        "    return adj_out_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in c:\\users\\kavya\\anaconda3\\lib\\site-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\kavya\\anaconda3\\lib\\site-packages (from scipy) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "a8qnC4BDaaz6",
        "outputId": "e1a2795e-22c0-477b-a978-d720db3a75ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing base training data from %s ... data_dir/training-mod-0.10.tsv\n",
            "['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>', 'a', 'aː', 'b', 'd', 'dʒ', 'd̪', 'e', 'g', 'gʱ', 'h', 'h̥', 'i', 'j', 'k', 'kʰ', 'l', 'lː', 'm', 'n', 'o', 'p', 'pʰ', 'pː', 'r', 's', 't', 'ts', 'tʃ', 'tʃʰ', 'tʰ', 't̪', 't̪ʰ', 'u', 'w', 'y', 'z', 'æ', 'ă', 'ĭ', 'ŋ', 'ŏ', 'ŭ', 'ɕ', 'ə', 'ɲ', 'ɾ', 'ʃ', 'ʋ', 'ʔ']\n",
            "HParams: %s {'embedding_dim': 16, 'kernel_width': 1, 'filters': 64, 'dropout': 0.5744652269043578, 'nonlinearity': 'leaky_relu', 'sfactor': 'conv'}\n",
            "Expanding training data ...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\kavya\\AppData\\Local\\Temp\\__autograph_generated_fileoyjh4sru.py\", line 10, in tf__call\n        output = ag__.converted_call(ag__.ld(graph_conv_op), (ag__.ld(input), ag__.ld(self).num_filters, ag__.ld(self).graph_conv_filters, ag__.ld(self).kernel), None, fscope)\n    File \"C:\\Users\\kavya\\AppData\\Local\\Temp\\__autograph_generated_file2k4wajqb.py\", line 46, in tf__graph_conv_op\n        ag__.if_stmt(ag__.converted_call(ag__.ld(len), (ag__.converted_call(ag__.ld(x).get_shape, (), None, fscope),), None, fscope) == 2, if_body_1, else_body_1, get_state_1, set_state_1, ('conv_op',), 1)\n    File \"C:\\Users\\kavya\\AppData\\Local\\Temp\\__autograph_generated_file2k4wajqb.py\", line 20, in if_body_1\n        conv_op = ag__.converted_call(ag__.ld(K).dot, (ag__.ld(graph_conv_filters), ag__.ld(x)), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'graph_cnn_8' (type GraphCNN).\n    \n    in user code:\n    \n        File \"c:\\Users\\kavya\\Downloads\\Project_GCNN 2\\Project_GCNN 2\\Project_GCNN\\keras_dgl\\layers\\graph_cnn_layer.py\", line 67, in call  *\n            output = graph_conv_op(input, self.num_filters, self.graph_conv_filters, self.kernel)\n        File \"c:\\Users\\kavya\\Downloads\\Project_GCNN 2\\Project_GCNN 2\\Project_GCNN\\keras_dgl\\layers\\graph_ops.py\", line 8, in graph_conv_op  *\n            conv_op = K.dot(graph_conv_filters, x)\n        File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 2455, in dot\n            out = tf.matmul(x, y)\n    \n        TypeError: Input 'b' of 'MatMul' Op has type int32 that does not match type float32 of argument 'a'.\n    \n    \n    Call arguments received by layer 'graph_cnn_8' (type GraphCNN):\n      • input=tf.Tensor(shape=(8, 20), dtype=int32)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 232\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[39m#       vocab_written = True\u001b[39;00m\n\u001b[0;32m    224\u001b[0m         \u001b[39m#   best_error = mean_accuracy\u001b[39;00m\n\u001b[0;32m    225\u001b[0m         \u001b[39m# print(best_error, mean_accuracy, '\\n')\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m     \u001b[39m# For some reason this step takes a couple of minutes to complete using\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[39m# Tensorflow 2.8.0.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mDone. Shutting down ...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m train_model()\n",
            "Cell \u001b[1;32mIn[24], line 170\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m     input_mask \u001b[39m=\u001b[39m inp_mask\n\u001b[0;32m    169\u001b[0m     target_mask \u001b[39m=\u001b[39m targ_mask\n\u001b[1;32m--> 170\u001b[0m model\u001b[39m.\u001b[39;49mfit(\u001b[39minput\u001b[39;49m, target, batch_size\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    171\u001b[0m Y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(inp, batch_size\u001b[39m=\u001b[39minput_mask\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m    172\u001b[0m index \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filejh6g2cib.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileoyjh4sru.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(graph_conv_op), (ag__\u001b[39m.\u001b[39mld(\u001b[39minput\u001b[39m), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_filters, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mgraph_conv_filters, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mkernel), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m():\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m (output,)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2k4wajqb.py:46\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__graph_conv_op\u001b[1;34m(x, num_filters, graph_conv_filters, kernel)\u001b[0m\n\u001b[0;32m     44\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlen\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mget_shape, (), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mconv_op\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m     45\u001b[0m conv_op \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mconv_op\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlen\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mget_shape, (), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39m\u001b[39mconv_op\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m conv_out \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(K)\u001b[39m.\u001b[39mdot, (ag__\u001b[39m.\u001b[39mld(conv_op), ag__\u001b[39m.\u001b[39mld(kernel)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     48\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2k4wajqb.py:20\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__graph_conv_op.<locals>.if_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_1\u001b[39m():\n\u001b[0;32m     19\u001b[0m     \u001b[39mnonlocal\u001b[39;00m conv_op\n\u001b[1;32m---> 20\u001b[0m     conv_op \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(K)\u001b[39m.\u001b[39;49mdot, (ag__\u001b[39m.\u001b[39;49mld(graph_conv_filters), ag__\u001b[39m.\u001b[39;49mld(x)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     21\u001b[0m     conv_op \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39msplit, (ag__\u001b[39m.\u001b[39mld(conv_op), ag__\u001b[39m.\u001b[39mld(num_filters)), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), fscope)\n\u001b[0;32m     22\u001b[0m     conv_op \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(K)\u001b[39m.\u001b[39mconcatenate, (ag__\u001b[39m.\u001b[39mld(conv_op),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope)\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\kavya\\AppData\\Local\\Temp\\__autograph_generated_fileoyjh4sru.py\", line 10, in tf__call\n        output = ag__.converted_call(ag__.ld(graph_conv_op), (ag__.ld(input), ag__.ld(self).num_filters, ag__.ld(self).graph_conv_filters, ag__.ld(self).kernel), None, fscope)\n    File \"C:\\Users\\kavya\\AppData\\Local\\Temp\\__autograph_generated_file2k4wajqb.py\", line 46, in tf__graph_conv_op\n        ag__.if_stmt(ag__.converted_call(ag__.ld(len), (ag__.converted_call(ag__.ld(x).get_shape, (), None, fscope),), None, fscope) == 2, if_body_1, else_body_1, get_state_1, set_state_1, ('conv_op',), 1)\n    File \"C:\\Users\\kavya\\AppData\\Local\\Temp\\__autograph_generated_file2k4wajqb.py\", line 20, in if_body_1\n        conv_op = ag__.converted_call(ag__.ld(K).dot, (ag__.ld(graph_conv_filters), ag__.ld(x)), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'graph_cnn_8' (type GraphCNN).\n    \n    in user code:\n    \n        File \"c:\\Users\\kavya\\Downloads\\Project_GCNN 2\\Project_GCNN 2\\Project_GCNN\\keras_dgl\\layers\\graph_cnn_layer.py\", line 67, in call  *\n            output = graph_conv_op(input, self.num_filters, self.graph_conv_filters, self.kernel)\n        File \"c:\\Users\\kavya\\Downloads\\Project_GCNN 2\\Project_GCNN 2\\Project_GCNN\\keras_dgl\\layers\\graph_ops.py\", line 8, in graph_conv_op  *\n            conv_op = K.dot(graph_conv_filters, x)\n        File \"c:\\Users\\kavya\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 2455, in dot\n            out = tf.matmul(x, y)\n    \n        TypeError: Input 'b' of 'MatMul' Op has type int32 that does not match type float32 of argument 'a'.\n    \n    \n    Call arguments received by layer 'graph_cnn_8' (type GraphCNN):\n      • input=tf.Tensor(shape=(8, 20), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from numpy import shape\n",
        "\n",
        "from keras_dgl.utils import *\n",
        "from keras_dgl.layers import GraphCNN\n",
        "import keras\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def normalize_adj(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
        "    else:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj).tocsr()\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def normalize_adj_numpy(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d)\n",
        "    else:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj)\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, symmetric=True):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_numpy(adj, symmetric=True):\n",
        "    adj = adj + np.eye(len(adj))\n",
        "    adj = normalize_adj_numpy(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
        "    adj_out_tensor = []\n",
        "    for i in range(adj_tensor.shape[0]):\n",
        "        adj = adj_tensor[i]\n",
        "        adj = adj + np.eye(adj.shape[0])\n",
        "        adj = normalize_adj_numpy(adj, symmetric)\n",
        "        adj_out_tensor.append(adj)\n",
        "    adj_out_tensor = np.array(adj_out_tensor)\n",
        "    return adj_out_tensor\n",
        "\n",
        "\n",
        "def categorical_crossentropy(preds, labels):\n",
        "    return np.mean(np.extract(labels, preds))\n",
        "\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
        "\n",
        "\n",
        "def evaluate_preds(preds, labels, indices):\n",
        "    split_loss = list()\n",
        "    split_acc = list()\n",
        "    for y_split, idx_split in zip(labels, indices):\n",
        "        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
        "        # split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
        "\n",
        "    return np.average(split_loss)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Training pipeline.\"\"\"\n",
        "\n",
        "    # Produce base training data and vocab, and expand the training data.\n",
        "    datadir = '/data_dir'\n",
        "    cogsets, vocab = build_base_data_and_vocab()\n",
        "    print(vocab)\n",
        "    char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "    idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "    nlangs = len(cogsets[0])\n",
        "    hparams = get_hparams()\n",
        "    vocab_size = len(vocab)\n",
        "    all_samples = expand_training_set(cogsets)\n",
        "\n",
        "    # # Read in the dev data.\n",
        "    # dev_sets = []\n",
        "    # filepath = '/content/data_dir/dev-0.10_01.tsv'\n",
        "    # with open(filepath, 'r', encoding='utf-8') as fin:\n",
        "    #   # Skip header.\n",
        "    #   next(fin)\n",
        "    #   for line in fin:\n",
        "    #     parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "    #     parts = ['<TARGET>' if p == '?' else p for p in parts]\n",
        "    #     dev_sets.append(parts)\n",
        "\n",
        "    # # Read in the dev  solution set.\n",
        "    # dev_solutions = []\n",
        "    # filepath = '/content/data_dir/dev_solutions-0.10_01.tsv'\n",
        "    # with open(filepath, 'r', encoding='utf-8') as fin:\n",
        "    #   # Skip header.\n",
        "    #   next(fin)\n",
        "    #   for line in fin:\n",
        "    #     parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "    #     dev_solutions.append(''.join(parts).strip())\n",
        "\n",
        "    # Core settings.\n",
        "    steps_per_epoch = 500\n",
        "    batch_size = 1\n",
        "    max_length = 20\n",
        "    # Have we written the vocab and hparams already?\n",
        "    vocab_written = False\n",
        "\n",
        "    # Define the model, optimizer and loss function.\n",
        "    # infiller = Infiller(vocab_size, hparams, batch_size, nlangs, max_length)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    checkpoint_dir = '/checkpoint_dir'\n",
        "    # if checkpoint_dir:\n",
        "    #     # checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "    #     checkpoint = tf.train.Checkpoint(optimizer=optimizer, infiller=infiller)\n",
        "\n",
        "    logging.info('Training the model ...')\n",
        "    train_dataset = build_train_dataset(all_samples, batch_size, nlangs,\n",
        "                                        max_length, char2idx)\n",
        "    best_error = None\n",
        "\n",
        "    SYM_NORM = True\n",
        "    vocab_size = vocab_size\n",
        "    embedding_dim = hparams['embedding_dim']\n",
        "    embedding = np.random.randint(500, size=(nlangs, vocab_size))\n",
        "    arr = []\n",
        "    for i in range(len(embedding)):\n",
        "        c = 0\n",
        "        a = []\n",
        "        for j in range(len(embedding)):\n",
        "            cosine = np.dot(embedding[i], embedding[j]) / (norm(embedding[i]) * norm(embedding[j]))\n",
        "            a.append(cosine)\n",
        "        arr.append(a)\n",
        "    A_norm = preprocess_adj_numpy(arr, SYM_NORM)\n",
        "    num_filters = 2\n",
        "    graph_conv_filters = np.concatenate([A_norm, np.matmul(A_norm, A_norm)], axis=0)\n",
        "    graph_conv_filters = K.constant(graph_conv_filters)\n",
        "    #   print(shape(graph_conv_filters))\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    nb_epochs = 10\n",
        "    model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['acc'])\n",
        "    for epoch in range(nb_epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for (_, (inp, targ, inp_mask,\n",
        "                 targ_mask)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "            input = inp\n",
        "            target = targ\n",
        "            input_mask = inp_mask\n",
        "            target_mask = targ_mask\n",
        "        model.fit(input, target, batch_size=input.shape[0], epochs=1, shuffle=False, verbose=0)\n",
        "        Y_pred = model.predict(inp, batch_size=input_mask.shape[1])\n",
        "        index = []\n",
        "        for i in range(0, len(target)):\n",
        "            index.append(i)\n",
        "        loss = evaluate_preds(Y_pred, target_mask, index)\n",
        "        # _, test_acc = evaluate_preds(Y_pred, [targ], [test_idx])\n",
        "\n",
        "        # batch_loss = train_step(infiller, optimizer, inp, inp_mask, targ,\n",
        "        #                         targ_mask)\n",
        "        print(\"Epoch: \", epoch, \"loss:\", loss)\n",
        "\n",
        "        # print(\"Epoch: {:04d}\".format(epoch), \"train_acc= {:.4f}\".format(train_loss))\n",
        "        # print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "        # total_loss / steps_per_epoch))\n",
        "\n",
        "        # print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "        # Evaluate on dev set:\n",
        "        # derrors = [0 for l in range(nlangs)]\n",
        "        # dtotals = [0 for l in range(nlangs)]\n",
        "        # allerrors = 0\n",
        "        # for dset, dsol in zip(dev_sets, dev_solutions):\n",
        "        #   tgt_index = dset.index('<TARGET>')\n",
        "        #   dtotals[tgt_index] += 1\n",
        "        #   pred = silent_translate(infiller, dset, char2idx, max_length, idx2char)\n",
        "        #   if pred != dsol:\n",
        "        #     derrors[tgt_index] += 1\n",
        "        #     allerrors += 1\n",
        "        # derrors = [x / y for x, y in zip(derrors, dtotals) if y != 0]\n",
        "        # mean_accuracy = np.mean(derrors)\n",
        "\n",
        "        # Update based on dev set.\n",
        "        # if not best_error or mean_accuracy <= best_error:\n",
        "        #   print('ERROR_UPDATE:', derrors)\n",
        "        # if checkpoint_dir:\n",
        "        #   checkpoint.save('/checkpoint_dir/best_model.ckpt')\n",
        "        #     # Write the vocab AFTER ensuring checkpoint dir has been created.\n",
        "        if not vocab_written:\n",
        "            # Write the model parameters.\n",
        "            hparams = get_hparams()\n",
        "            hparams['embedding_dim'] = hparams[\"embedding_dim\"]\n",
        "            hparams['kernel_width'] = hparams[\"kernel_width\"]\n",
        "            hparams['filters'] = hparams[\"filters\"]\n",
        "            hparams['dropout'] = hparams[\"dropout\"]\n",
        "            hparams['nonlinearity'] = hparams[\"nonlinearity\"]\n",
        "            hparams['sfactor'] = hparams[\"sfactor\"]\n",
        "            with open('checkpoint_dir/' + 'hparams.json', 'w') as vfile:\n",
        "                json.dump(hparams, vfile)\n",
        "            #       # Write the vocabulary.\n",
        "            with open('checkpoint_dir/' + '/vocab.txt', 'w', encoding='utf-8') as vfile:\n",
        "                for v in vocab:\n",
        "                    vfile.write(v + '\\n')\n",
        "        #       vocab_written = True\n",
        "        #   best_error = mean_accuracy\n",
        "        # print(best_error, mean_accuracy, '\\n')\n",
        "\n",
        "    # For some reason this step takes a couple of minutes to complete using\n",
        "    # Tensorflow 2.8.0.\n",
        "    logging.info('Done. Shutting down ...')\n",
        "\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "a8qnC4BDaaz6",
        "outputId": "e1a2795e-22c0-477b-a978-d720db3a75ed"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Dense, Activation, Dropout\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.regularizers import l2\n",
        "# from keras.optimizers import Adam\n",
        "# import keras.backend as K\n",
        "# import numpy as np\n",
        "# from numpy import shape\n",
        "\n",
        "# from keras_dgl.utils import *\n",
        "# from keras_dgl.layers import GraphCNN\n",
        "# import keras\n",
        "# import numpy as np\n",
        "# from numpy.linalg import norm\n",
        "\n",
        "# def silent_translate(infiller, cset, char2idx, max_length, idx2char):\n",
        "#   result = evaluate_cset(infiller, cset, char2idx, max_length)\n",
        "#   #result = list(result)\n",
        "#   result = ' '.join([\n",
        "#       idx2char[x] for x in result if idx2char[x] not in\n",
        "#       ['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>']\n",
        "#   ])\n",
        "#   #print(\"RESULT\",result,\"\\n\")\n",
        "#   return result\n",
        "  \n",
        "# def normalize_adj(adj, symmetric=True):\n",
        "#     if symmetric:\n",
        "#         d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "#         a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
        "#     else:\n",
        "#         d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "#         a_norm = d.dot(adj).tocsr()\n",
        "#     return a_norm\n",
        "\n",
        "\n",
        "# def normalize_adj_numpy(adj, symmetric=True):\n",
        "#     if symmetric:\n",
        "#         d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "#         a_norm = adj.dot(d).transpose().dot(d)\n",
        "#     else:\n",
        "#         d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "#         a_norm = d.dot(adj)\n",
        "#     return a_norm\n",
        "\n",
        "\n",
        "# def preprocess_adj(adj, symmetric=True):\n",
        "#     adj = adj + sp.eye(adj.shape[0])\n",
        "#     adj = normalize_adj(adj, symmetric)\n",
        "#     return adj\n",
        "\n",
        "\n",
        "# def preprocess_adj_numpy(adj, symmetric=True):\n",
        "#     adj = adj + np.eye(len(adj))\n",
        "#     adj = normalize_adj_numpy(adj, symmetric)\n",
        "#     return adj\n",
        "\n",
        "\n",
        "# def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
        "#     adj_out_tensor = []\n",
        "#     for i in range(adj_tensor.shape[0]):\n",
        "#         adj = adj_tensor[i]\n",
        "#         adj = adj + np.eye(adj.shape[0])\n",
        "#         adj = normalize_adj_numpy(adj, symmetric)\n",
        "#         adj_out_tensor.append(adj)\n",
        "#     adj_out_tensor = np.array(adj_out_tensor)\n",
        "#     return adj_out_tensor\n",
        "\n",
        "\n",
        "# def categorical_crossentropy(preds, labels):\n",
        "#     return np.mean(np.extract(labels, preds))\n",
        "\n",
        "\n",
        "# def accuracy(preds, labels):\n",
        "#     return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
        "\n",
        "\n",
        "# def evaluate_preds(preds, labels, indices):\n",
        "#     split_loss = list()\n",
        "#     split_acc = list()\n",
        "#     for y_split, idx_split in zip(labels, indices):\n",
        "#         split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
        "#         # split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
        "\n",
        "#     return np.average(split_loss)\n",
        "\n",
        "# def get_vocab(checkpoint_dir):\n",
        "#   file_path =\"checkpoint_dir/vocab.txt\"\n",
        "#   if not file_path:\n",
        "#     raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "#   logging.info('Loading vocab from %s ...', file_path)\n",
        "#   with open(file_path, 'r', encoding='utf8') as f:\n",
        "#     vocab = [symbol.strip() for symbol in f if symbol]\n",
        "#   logging.info('%d symbols loaded.', len(vocab))\n",
        "#   return vocab\n",
        "\n",
        "# def decode_with_model(Y_pred,model):\n",
        "#   hparams = get_hparams()\n",
        "#   checkpoint_dir='checkpoint_dir/'\n",
        "#   vocab = get_vocab(\"checkpoint_dir/\")\n",
        "#   char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "#   idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "#   vocab_size = len(vocab)\n",
        "#   batch_size = 1\n",
        "#   max_length = 20\n",
        "\n",
        "#   test_filepath = 'data_dir/test-0.10.tsv'\n",
        "#   preds_filepath = 'data_dir/pred-0.10.tsv'\n",
        "\n",
        "#   with open(test_filepath, 'r', encoding='utf-8') as fin:\n",
        "#     nlangs = len(next(fin).strip('\\n').split('\\t')) - 1\n",
        "\n",
        "\n",
        "#   latest_ckpt_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "#   if not latest_ckpt_path:\n",
        "#     raise ValueError('No checkpoint available')\n",
        "#   logging.info('Restoring from checkpoint %s ...', latest_ckpt_path)\n",
        "#   infiller = model\n",
        "#   checkpoint = tf.train.Checkpoint(model)\n",
        "#   checkpoint.restore(latest_ckpt_path).expect_partial()\n",
        "\n",
        "#   logging.info('Generating predictions and saving results...')\n",
        "#   with open(preds_filepath, 'w', encoding='utf-8') as vfile:\n",
        "#     with open(test_filepath, 'r', encoding='utf-8') as tfile:\n",
        "#       # Copy the header.\n",
        "#       vfile.write(next(tfile))\n",
        "#       for line in tfile:\n",
        "#         parts = line.strip('\\n').split('\\t')\n",
        "#         tset = ['<TARGET>' if p == '?' else p for p in parts[1:]]\n",
        "#         tgt_index = tset.index('<TARGET>')\n",
        "#         pred = silent_translate(Y_pred, tset, char2idx, max_length, idx2char)\n",
        "#         row = ['' for p in parts]\n",
        "#         row[0] = parts[0]\n",
        "#         row[tgt_index + 1] = pred\n",
        "#         vfile.write('\\t'.join(row) + '\\n')\n",
        "\n",
        "\n",
        "# def train_model():\n",
        "#     \"\"\"Training pipeline.\"\"\"\n",
        "\n",
        "#     # Produce base training data and vocab, and expand the training data.\n",
        "#     datadir = '/data_dir'\n",
        "#     cogsets, vocab = build_base_data_and_vocab()\n",
        "#     print(vocab)\n",
        "#     char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "#     idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "#     nlangs = len(cogsets[0])\n",
        "#     hparams = get_hparams()\n",
        "#     vocab_size = len(vocab)\n",
        "#     all_samples = expand_training_set(cogsets)\n",
        "\n",
        "#     # # Read in the dev data.\n",
        "#     # dev_sets = []\n",
        "#     # filepath = '/content/data_dir/dev-0.10_01.tsv'\n",
        "#     # with open(filepath, 'r', encoding='utf-8') as fin:\n",
        "#     #   # Skip header.\n",
        "#     #   next(fin)\n",
        "#     #   for line in fin:\n",
        "#     #     parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "#     #     parts = ['<TARGET>' if p == '?' else p for p in parts]\n",
        "#     #     dev_sets.append(parts)\n",
        "\n",
        "#     # # Read in the dev  solution set.\n",
        "#     # dev_solutions = []\n",
        "#     # filepath = '/content/data_dir/dev_solutions-0.10_01.tsv'\n",
        "#     # with open(filepath, 'r', encoding='utf-8') as fin:\n",
        "#     #   # Skip header.\n",
        "#     #   next(fin)\n",
        "#     #   for line in fin:\n",
        "#     #     parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "#     #     dev_solutions.append(''.join(parts).strip())\n",
        "\n",
        "#     # Core settings.\n",
        "#     steps_per_epoch = 500\n",
        "#     batch_size = 1\n",
        "#     max_length = 20\n",
        "#     # Have we written the vocab and hparams already?\n",
        "#     vocab_written = False\n",
        "\n",
        "#     # Define the model, optimizer and loss function.\n",
        "#     # infiller = Infiller(vocab_size, hparams, batch_size, nlangs, max_length)\n",
        "#     optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "#     checkpoint_dir = '/checkpoint_dir'\n",
        "#     # if checkpoint_dir:\n",
        "#     #     # checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "#     #     checkpoint = tf.train.Checkpoint(optimizer=optimizer, infiller=infiller)\n",
        "\n",
        "#     logging.info('Training the model ...')\n",
        "#     train_dataset = build_train_dataset(all_samples, batch_size, nlangs,\n",
        "#                                         max_length, char2idx)\n",
        "#     best_error = None\n",
        "\n",
        "#     SYM_NORM = True\n",
        "#     vocab_size = vocab_size\n",
        "#     embedding_dim = hparams['embedding_dim']\n",
        "#     embedding = np.random.randint(500, size=(nlangs, vocab_size))\n",
        "#     arr = []\n",
        "#     for i in range(len(embedding)):\n",
        "#         c = 0\n",
        "#         a = []\n",
        "#         for j in range(len(embedding)):\n",
        "#             cosine = np.dot(embedding[i], embedding[j]) / (norm(embedding[i]) * norm(embedding[j]))\n",
        "#             a.append(cosine)\n",
        "#         arr.append(a)\n",
        "#     A_norm = preprocess_adj_numpy(arr, SYM_NORM)\n",
        "#     num_filters = 2\n",
        "#     graph_conv_filters = np.concatenate([A_norm, np.matmul(A_norm, A_norm)], axis=0)\n",
        "#     graph_conv_filters = K.constant(graph_conv_filters)\n",
        "#     #   print(shape(graph_conv_filters))\n",
        "\n",
        "#     model = Sequential()\n",
        "\n",
        "#     nb_epochs = 10\n",
        "#     model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "#     model.add(Activation('tanh'))\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
        "#     for epoch in range(nb_epochs):\n",
        "#         start = time.time()\n",
        "\n",
        "#         total_loss = 0\n",
        "\n",
        "#         for (_, (inp, targ, inp_mask,\n",
        "#                  targ_mask)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "#             input_i = tf.reshape(inp, [inp.shape[1], inp.shape[2]], name=None)\n",
        "#             targ_i = tf.reshape(targ, [targ.shape[1], targ.shape[2]], name=None)\n",
        "#             # print(shape(input_i))\n",
        "#             # print(shape(targ_i))\n",
        "#             input_mask = tf.reshape(inp_mask, [inp_mask.shape[1], inp_mask.shape[2]], name=None)\n",
        "#             target_mask = tf.reshape(targ_mask, [targ_mask.shape[1], targ_mask.shape[2]], name=None)\n",
        "#             # model.build(shape(input_i))\n",
        "#             # model.summary()\n",
        "#         model.fit(input_i, targ_i, batch_size=input_i.shape[0], epochs=1, shuffle=False, verbose=0)\n",
        "#         Y_pred = model.predict(input_mask, batch_size=inp_mask.shape[1])\n",
        "#         index = []\n",
        "#         for i in range(0, len(target_mask)):\n",
        "#             index.append(i)\n",
        "#         loss = evaluate_preds(Y_pred, targ_mask, index)\n",
        "#         # _, test_acc = evaluate_preds(Y_pred, [targ], [test_idx])\n",
        "\n",
        "#         # batch_loss = train_step(infiller, optimizer, inp, inp_mask, targ,\n",
        "#         #                         targ_mask)\n",
        "#         print(\"Epoch: \", epoch, \"loss:\", loss)\n",
        "        \n",
        "#         checkpoint = tf.train.Checkpoint( model)\n",
        "#         checkpoint.save('checkpoint_dir/best_model.ckpt')\n",
        "#         model1=model\n",
        "#         # print(\"Epoch: {:04d}\".format(epoch), \"train_acc= {:.4f}\".format(train_loss))\n",
        "#         # print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "#         # total_loss / steps_per_epoch))\n",
        "\n",
        "#         # print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "#         # Evaluate on dev set:\n",
        "#         # derrors = [0 for l in range(nlangs)]\n",
        "#         # dtotals = [0 for l in range(nlangs)]\n",
        "#         # allerrors = 0\n",
        "#         # for dset, dsol in zip(dev_sets, dev_solutions):\n",
        "#         #   tgt_index = dset.index('<TARGET>')\n",
        "#         #   dtotals[tgt_index] += 1\n",
        "#         #   pred = silent_translate(infiller, dset, char2idx, max_length, idx2char)\n",
        "#         #   if pred != dsol:\n",
        "#         #     derrors[tgt_index] += 1\n",
        "#         #     allerrors += 1\n",
        "#         # derrors = [x / y for x, y in zip(derrors, dtotals) if y != 0]\n",
        "#         # mean_accuracy = np.mean(derrors)\n",
        "\n",
        "#         # Update based on dev set.\n",
        "#         # if not best_error or mean_accuracy <= best_error:\n",
        "#         #   print('ERROR_UPDATE:', derrors)\n",
        "#         # if checkpoint_dir:\n",
        "#         #   checkpoint.save('/checkpoint_dir/best_model.ckpt')\n",
        "#         #     # Write the vocab AFTER ensuring checkpoint dir has been created.\n",
        "#         if not vocab_written:\n",
        "#             # Write the model parameters.\n",
        "#             hparams = get_hparams()\n",
        "#             hparams['embedding_dim'] = hparams[\"embedding_dim\"]\n",
        "#             hparams['kernel_width'] = hparams[\"kernel_width\"]\n",
        "#             hparams['filters'] = hparams[\"filters\"]\n",
        "#             hparams['dropout'] = hparams[\"dropout\"]\n",
        "#             hparams['nonlinearity'] = hparams[\"nonlinearity\"]\n",
        "#             hparams['sfactor'] = hparams[\"sfactor\"]\n",
        "#             with open('checkpoint_dir/' + 'hparams.json', 'w') as vfile:\n",
        "#                 json.dump(hparams, vfile)\n",
        "#             #       # Write the vocabulary.\n",
        "#             with open('checkpoint_dir/' + '/vocab.txt', 'w', encoding='utf-8') as vfile:\n",
        "#                 for v in vocab:\n",
        "#                     vfile.write(v + '\\n')\n",
        "#         #       vocab_written = True\n",
        "#         #   best_error = mean_accuracy\n",
        "#         # print(best_error, mean_accuracy, '\\n')\n",
        "#     decode_with_model(Y_pred,model)\n",
        "#     # For some reason this step takes a couple of minutes to complete using\n",
        "#     # Tensorflow 2.8.0.\n",
        "#     logging.info('Done. Shutting down ...')\n",
        "\n",
        "\n",
        "# train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ilsGIsTnaDg"
      },
      "outputs": [],
      "source": [
        "def get_vocab(checkpoint_dir):\n",
        "  file_path =\"checkpoint_dir/vocab.txt\"\n",
        "  if not file_path:\n",
        "    raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "  logging.info('Loading vocab from %s ...', file_path)\n",
        "  with open(file_path, 'r', encoding='utf8') as f:\n",
        "    vocab = [symbol.strip() for symbol in f if symbol]\n",
        "  logging.info('%d symbols loaded.', len(vocab))\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "093fZ15zcRXM"
      },
      "outputs": [],
      "source": [
        "def decode_with_model(model):\n",
        "  hparams = get_hparams()\n",
        "  checkpoint_dir='checkpoint_dir/'\n",
        "  vocab = get_vocab(\"checkpoint_dir/\")\n",
        "  char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "  idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "  vocab_size = len(vocab)\n",
        "  batch_size = 1\n",
        "  max_length = 20\n",
        "\n",
        "  test_filepath = 'data_dir/test-0.10.tsv'\n",
        "  preds_filepath = 'data_dir/pred-0.10.tsv'\n",
        "\n",
        "  with open(test_filepath, 'r', encoding='utf-8') as fin:\n",
        "    nlangs = len(next(fin).strip('\\n').split('\\t')) - 1\n",
        "\n",
        "\n",
        "  latest_ckpt_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  if not latest_ckpt_path:\n",
        "    raise ValueError('No checkpoint available')\n",
        "  logging.info('Restoring from checkpoint %s ...', latest_ckpt_path)\n",
        "  infiller = model\n",
        "  checkpoint = tf.train.Checkpoint(model)\n",
        "  checkpoint.restore(latest_ckpt_path).expect_partial()\n",
        "\n",
        "  logging.info('Generating predictions and saving results...')\n",
        "  with open(preds_filepath, 'w', encoding='utf-8') as vfile:\n",
        "    with open(test_filepath, 'r', encoding='utf-8') as tfile:\n",
        "      # Copy the header.\n",
        "      vfile.write(next(tfile))\n",
        "      for line in tfile:\n",
        "        parts = line.strip('\\n').split('\\t')\n",
        "        tset = ['<TARGET>' if p == '?' else p for p in parts[1:]]\n",
        "        tgt_index = tset.index('<TARGET>')\n",
        "        pred = silent_translate(infiller, tset, char2idx, max_length, idx2char)\n",
        "        row = ['' for p in parts]\n",
        "        row[0] = parts[0]\n",
        "        row[tgt_index + 1] = pred\n",
        "        vfile.write('\\t'.join(row) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8j_UY07tvyn",
        "outputId": "491e7b49-28f4-411b-9bcf-3a6b7b05b721"
      },
      "outputs": [],
      "source": [
        "%pip install lingrex\n",
        "%pip install lingpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFQTxwu8_83x",
        "outputId": "fba22c2a-b776-4475-e1ea-a307e6e3f98e"
      },
      "outputs": [],
      "source": [
        "from lingrex.util import bleu_score\n",
        "from lingpy import *\n",
        "from lingpy.evaluate.acd import _get_bcubed_score as bcubed_score\n",
        "from tabulate import tabulate\n",
        "from collections import defaultdict\n",
        "from lingpy.sequence.ngrams import get_n_ngrams\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2dTmkLgPtzmL"
      },
      "outputs": [],
      "source": [
        "def load_cognate_file(path):\n",
        "    \"\"\"\n",
        "    Helper function for simplified cognate formats.\n",
        "    \"\"\"\n",
        "    data = csv2list(path, strip_lines=False)\n",
        "    header = data[0]\n",
        "    languages = header[1:]\n",
        "    out = {}\n",
        "    sounds = defaultdict(lambda : defaultdict(list))\n",
        "    for row in data[1:]:\n",
        "        out[row[0]] = {}\n",
        "        for language, entry in zip(languages, row[1:]):\n",
        "            out[row[0]][language] = entry.split()\n",
        "            for i, sound in enumerate(entry.split()):\n",
        "                sounds[sound][language] += [[row[0], i]]\n",
        "    # print(\"Languages:::::\",languages)\n",
        "    # print(\"Sound::::\",sounds)\n",
        "    # print(\"OUT::::\",out)\n",
        "    return languages,sounds, out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TwVKRhbjt3Um"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def bleu_score(word, reference, n=4, weights=None, trim=False):\n",
        "    \"\"\"\n",
        "    Compute the BLEU score for predicted word and reference.\n",
        "    \"\"\"\n",
        "\n",
        "    if not weights:\n",
        "        weights = [1 / n for x in range(n)]\n",
        "\n",
        "    scores = []\n",
        "    for i in range(1, n + 1):\n",
        "\n",
        "        new_wrd = list(get_n_ngrams(word, i))\n",
        "        new_ref = list(get_n_ngrams(reference, i))\n",
        "        if trim and i > 1:\n",
        "            new_wrd = new_wrd[i - 1 : -(i - 1)]\n",
        "            new_ref = new_ref[i - 1 : -(i - 1)]\n",
        "\n",
        "        clipped, divide = [], []\n",
        "        for itm in set(new_wrd):\n",
        "            clipped += [new_ref.count(itm)]\n",
        "            divide += [new_wrd.count(itm)]\n",
        "        scores += [sum(clipped) / sum(divide)]\n",
        "\n",
        "    # calculate arithmetic mean\n",
        "    out_score = 1\n",
        "    for weight, score in zip(weights, scores):\n",
        "        out_score = out_score * (score**weight)\n",
        "\n",
        "    if len(word) > len(reference):\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.e ** (1 - (len(reference) / len(word)))\n",
        "    return bp * (out_score ** (1 / sum(weights)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9h8nLazmAKig"
      },
      "outputs": [],
      "source": [
        "def compare_words(firstfile, secondfile, report=True):\n",
        "    \"\"\"\n",
        "    Evaluate the predicted and attested words in two datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    (languages, soundsA, first), (languagesB, soundsB, last) = load_cognate_file(firstfile), load_cognate_file(secondfile)\n",
        "    print(\"///\",languages, soundsA, first)\n",
        "    all_scores = []\n",
        "    for language in languages:\n",
        "        scores = []\n",
        "        almsA, almsB = [], []\n",
        "        for key in first:\n",
        "            if language in first[key]:\n",
        "                entryA = first[key][language]\n",
        "                # print(\"@@@@\",entryA)\n",
        "                if \" \".join(entryA):\n",
        "                    try:\n",
        "                        # print(\"&&&&\",entryA)\n",
        "                        entryB = last[key][language]\n",
        "                        # print(\"####\",entryB)\n",
        "                    except KeyError:\n",
        "                        print(\"Missing entry {0} / {1} / {2}\".format(\n",
        "                            key, language, secondfile))\n",
        "                        entryB = \"\"\n",
        "                    if not entryB:\n",
        "                        entryB = (2 * len(entryA)) * [\"Ø\"]\n",
        "                    # print(entryA)\n",
        "                    # print(entryB)\n",
        "                    almA, almB, _ = nw_align(entryA, entryB)\n",
        "                    almsA += almA\n",
        "                    almsB += almB\n",
        "                    score = 0\n",
        "                    for a, b in zip(almA, almB):\n",
        "                        if a == b and a not in \"Ø?-\":\n",
        "                            pass\n",
        "                        elif a != b:\n",
        "                            score += 1\n",
        "                    scoreD = score / len(almA)\n",
        "                    bleu = bleu_score(entryA, entryB, n=4, trim=False)\n",
        "                    scores += [[key, entryA, entryB, score, scoreD, bleu]]\n",
        "        if scores:\n",
        "            p, r = bcubed_score(almsA, almsB), bcubed_score(almsB, almsA)\n",
        "            fs = 2 * (p*r) / (p+r)\n",
        "            all_scores += [[\n",
        "                language,\n",
        "                sum([row[-3] for row in scores])/len(scores),\n",
        "                sum([row[-2] for row in scores])/len(scores),\n",
        "                fs,\n",
        "                sum([row[-1] for row in scores])/len(scores)]]\n",
        "    all_scores += [[\n",
        "        \"TOTAL\", \n",
        "        sum([row[-4] for row in all_scores])/len(languages),\n",
        "        sum([row[-3] for row in all_scores])/len(languages),\n",
        "        sum([row[-2] for row in all_scores])/len(languages),\n",
        "        sum([row[-1] for row in all_scores])/len(languages),\n",
        "        ]]\n",
        "    if report:\n",
        "        print(\n",
        "                tabulate(\n",
        "                    all_scores, \n",
        "                    headers=[\n",
        "                        \"Language\", \"ED\", \"ED (Normalized)\", \n",
        "                        \"B-Cubed FS\", \"BLEU\"], floatfmt=\".3f\"))\n",
        "    return all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MZReSik5AQAg"
      },
      "outputs": [],
      "source": [
        "# compare_words('/content/data_dir/pred-0.10.tsv','/content/data_dir/solutions-0.10.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40B-pZRQbqVW",
        "outputId": "d5d0e6c6-b1e7-440b-e996-3898312a51d2"
      },
      "outputs": [],
      "source": [
        "compare_words('/content/data_dir/pred-0.10.tsv','/content/data_dir/solutions-0.10.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRJhqqIqzOAQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "c86c62298c5bb419d4de87ad1ec456689eb9cadef83b8d0d1a7ee6d54e5e7304"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
