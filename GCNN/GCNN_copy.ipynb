{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "AEteXjRI2574",
        "outputId": "5b70f0ba-fb44-4e81-a8ab-4debab8a339d"
      },
      "outputs": [],
      "source": [
        "%pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exCg9pJCbWMO",
        "outputId": "add11ac3-87ca-4a89-99ef-031ec2c2db40"
      },
      "outputs": [],
      "source": [
        "%pip install keras-gcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install absl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%pip install absl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "# %pip uninstall tensorflow\n",
        "%pip install tensorflow\n",
        "# import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N90cMy8EYw6N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Sequence\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras_gcn import GraphConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kLWlTZ5gY9nu"
      },
      "outputs": [],
      "source": [
        "def build_base_data_and_vocab():\n",
        "  \"\"\"Builds vocabulary from the training data files.\"\"\"\n",
        "  vocab = set()\n",
        "  cognates = []\n",
        "  filepath = 'data_dir/training-mod-0.10.tsv'\n",
        "  print('Preparing base training data from %s ...', filepath)\n",
        "  with open(filepath, 'r', encoding='utf-8') as f:\n",
        "    # Skip header.\n",
        "    next(f)\n",
        "    for line in f:\n",
        "      parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "      for p in parts:\n",
        "        for c in p.split():\n",
        "          vocab.add(c)\n",
        "      cognates.append([p.strip() for p in parts])\n",
        "  vocab = ['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>'] + sorted(\n",
        "      list(vocab))\n",
        "  # print(\"COGSETS:\",cogsets,\"**********************\",\"\\n\")\n",
        "  # print(\"VOCAB:\",vocab,\"$$$$$$$$$$$$$$$$$$$$$$$\",\"\\n\")\n",
        "  return cognates, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nMfM0EGRZUzP"
      },
      "outputs": [],
      "source": [
        "def get_hparams():\n",
        "  \"\"\"Builds hyper-parameter dictionary from flags or file.\"\"\"\n",
        "  with open('checkpoint_dir/hparams.json', 'r') as f:\n",
        "    hparams = json.load(f)\n",
        "    print('HParams: %s', hparams)\n",
        "    # print(\"HPARAMS\",hparams,\"\\n\")\n",
        "    return hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zk7nibQeaDqc"
      },
      "outputs": [],
      "source": [
        "def expand_training_set(cogsets):\n",
        "  \"\"\"Expands the dataset to all possible variations.\"\"\"\n",
        "  print('Expanding training data ...')\n",
        "  nlangs = len(cogsets[0])\n",
        "  all_samples = []\n",
        "  for cs in cogsets:\n",
        "    # Find all valid positions.\n",
        "    isample = []\n",
        "    for i in range(nlangs):\n",
        "      if cs[i]:\n",
        "        isample.append(cs[i])\n",
        "      else:\n",
        "        isample.append('<BLANK>')\n",
        "    all_samples.append(isample)\n",
        "  random.shuffle(all_samples)\n",
        "  # print(\"ALL SAMPLES:\",all_samples,\"\\n\")\n",
        "  return all_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O-YTgUDhYxcS"
      },
      "outputs": [],
      "source": [
        "def build_train_dataset(all_samples, batch_size, nlangs, max_length, char2idx):\n",
        "  \"\"\"Creates train dataset from the generator.\"\"\"\n",
        "  # Create data generators to feed into the networks.\n",
        "  def la_gen():\n",
        "    while True:\n",
        "      for icset in all_samples:\n",
        "        inputs = []\n",
        "        targets = []\n",
        "        input_mask = []\n",
        "        target_mask = []\n",
        "        # Get the present items.\n",
        "        valids = [i for i in range(len(icset)) if icset[i] != '<BLANK>']\n",
        "        # Select how many inputs will be present to provide information.\n",
        "        num_present = random.randint(1, len(valids))\n",
        "        present = random.sample(valids, num_present)\n",
        "        # Create the actual data content.\n",
        "        for i in range(len(icset)):\n",
        "          # Create a max_length sequence.\n",
        "          template = [char2idx['<BLANK>']] * max_length\n",
        "          seq = [char2idx['<BOS>']] + [\n",
        "              char2idx[c] if c in char2idx else char2idx['<UNK>']\n",
        "              for c in icset[i].split()\n",
        "          ] + [char2idx['<EOS>']]\n",
        "          for j in range(min(len(seq), max_length)):\n",
        "            template[j] = seq[j]\n",
        "          targets.append(template)\n",
        "          inputs.append(template)\n",
        "          # If the sequence if valid, get gradient from it.\n",
        "          if i in valids:\n",
        "            target_mask.append([1.0] * max_length)\n",
        "          else:\n",
        "            target_mask.append([0.0] * max_length)\n",
        "          # If the sequence should be present, don't mask it.\n",
        "          if i in present:\n",
        "            input_mask.append([1.0] * max_length)\n",
        "          else:\n",
        "            input_mask.append([0.0] * max_length)\n",
        "        # Convert to required tensor formats.\n",
        "        inputs = tf.constant([inputs], dtype='float32')\n",
        "        targets = tf.constant([targets], dtype='float32')\n",
        "        input_mask = tf.constant([input_mask], dtype='float32')\n",
        "        target_mask = tf.constant([target_mask], dtype='float32')\n",
        "\n",
        "        yield (inputs, targets, input_mask, target_mask)\n",
        "\n",
        "  return tf.data.Dataset.from_generator(\n",
        "      la_gen,\n",
        "      output_signature=(tf.TensorSpec(\n",
        "          shape=(batch_size, nlangs, max_length), dtype='float32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(batch_size, nlangs, max_length),\n",
        "                            dtype='float32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(batch_size, nlangs, max_length),\n",
        "                            dtype='float32'),\n",
        "                        tf.TensorSpec(\n",
        "                            shape=(batch_size, nlangs, max_length),\n",
        "                            dtype='float32')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UiECAcY_bcci"
      },
      "outputs": [],
      "source": [
        "from numpy import shape\n",
        "def evaluate_cset(infiller, cset, char2idx, max_length):\n",
        "  \"\"\"Evaluates given cognate set.\"\"\"\n",
        "  tgt_index = 0\n",
        "  inputs = []\n",
        "  input_mask = []\n",
        "  # Find possible target positions\n",
        "  for i, p in enumerate(cset):\n",
        "    if p.strip():\n",
        "      if p == '<TARGET>':\n",
        "        tgt_index = i\n",
        "        inputs.append([char2idx['<TARGET>']] * max_length)\n",
        "        input_mask.append([0.0] * max_length)\n",
        "      else:\n",
        "        seq = [char2idx['<BOS>']] + [\n",
        "            char2idx[c] if c in char2idx else char2idx['<UNK>']\n",
        "            for c in p.split()\n",
        "        ] + [char2idx['<EOS>']]\n",
        "        template = [char2idx['<BLANK>']] * max_length\n",
        "        for j in range(min(len(seq), max_length)):\n",
        "          template[j] = seq[j]\n",
        "        inputs.append(template)\n",
        "        input_mask.append([1.0] * max_length)\n",
        "    else:\n",
        "      inputs.append([char2idx['<BLANK>']] * max_length)\n",
        "      input_mask.append([0.0] * max_length)\n",
        "\n",
        "  inputs = tf.constant([inputs], dtype='float32')\n",
        "  input_mask = tf.constant([input_mask], dtype='float32')\n",
        "\n",
        "  logits = infiller\n",
        "  trow = tf.math.argmax(logits[tgt_index,:], axis=-1)\n",
        "  return trow.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1WISb-SrvEt",
        "outputId": "6cb6ab86-9d3e-4db1-b80a-22f9527e3927"
      },
      "outputs": [],
      "source": [
        "%pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_adj(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
        "    else:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj).tocsr()\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def normalize_adj_numpy(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d)\n",
        "    else:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj)\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, symmetric=True):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_numpy(adj, symmetric=True):\n",
        "    adj = adj + np.eye(adj.shape[0])\n",
        "    adj = normalize_adj_numpy(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
        "    adj_out_tensor = []\n",
        "    for i in range(adj_tensor.shape[0]):\n",
        "        adj = adj_tensor[i]\n",
        "        adj = adj + np.eye(adj.shape[0])\n",
        "        adj = normalize_adj_numpy(adj, symmetric)\n",
        "        adj_out_tensor.append(adj)\n",
        "    adj_out_tensor = np.array(adj_out_tensor)\n",
        "    return adj_out_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in c:\\users\\kavya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.11.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "a8qnC4BDaaz6",
        "outputId": "e1a2795e-22c0-477b-a978-d720db3a75ed"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from numpy import shape\n",
        "\n",
        "from keras_dgl.utils import *\n",
        "from keras_dgl.layers import GraphCNN\n",
        "import keras\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def silent_translate(infiller, cset, char2idx, max_length, idx2char):\n",
        "  result = evaluate_cset(infiller, cset, char2idx, max_length)\n",
        "  result = result.tolist()\n",
        "  print(result)\n",
        "  result = ' '.join([idx2char[x] for x in result if idx2char[x] not in\n",
        "      ['<PAD>', '<EOS>', '<BOS>', '<UNK>', '<TARGET>', '<BLANK>']])\n",
        "  #print(\"RESULT\",result,\"\\n\")\n",
        "  return result\n",
        "  \n",
        "def normalize_adj(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
        "    else:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj).tocsr()\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def normalize_adj_numpy(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
        "        a_norm = adj.dot(d).transpose().dot(d)\n",
        "    else:\n",
        "        d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
        "        a_norm = d.dot(adj)\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, symmetric=True):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_numpy(adj, symmetric=True):\n",
        "    adj = adj + np.eye(len(adj))\n",
        "    adj = normalize_adj_numpy(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
        "    adj_out_tensor = []\n",
        "    for i in range(adj_tensor.shape[0]):\n",
        "        adj = adj_tensor[i]\n",
        "        adj = adj + np.eye(adj.shape[0])\n",
        "        adj = normalize_adj_numpy(adj, symmetric)\n",
        "        adj_out_tensor.append(adj)\n",
        "    adj_out_tensor = np.array(adj_out_tensor)\n",
        "    return adj_out_tensor\n",
        "\n",
        "\n",
        "def categorical_crossentropy(preds, labels):\n",
        "    return np.mean(np.extract(labels, preds))\n",
        "\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
        "\n",
        "\n",
        "def evaluate_preds(preds, labels, indices):\n",
        "    split_loss = list()\n",
        "    split_acc = list()\n",
        "    for y_split, idx_split in zip(labels, indices):\n",
        "        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
        "        # split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
        "\n",
        "    return np.average(split_loss)\n",
        "\n",
        "def get_vocab(checkpoint_dir):\n",
        "  file_path =\"checkpoint_dir/vocab.txt\"\n",
        "  if not file_path:\n",
        "    raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "  logging.info('Loading vocab from %s ...', file_path)\n",
        "  with open(file_path, 'r', encoding='utf8') as f:\n",
        "    vocab = [symbol.strip() for symbol in f if symbol]\n",
        "  logging.info('%d symbols loaded.', len(vocab))\n",
        "  return vocab\n",
        "\n",
        "def decode_with_model(Y_pred,model):\n",
        "  hparams = get_hparams()\n",
        "  checkpoint_dir='checkpoint_dir/'\n",
        "  vocab = get_vocab(\"checkpoint_dir/\")\n",
        "  char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "  idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "  vocab_size = len(vocab)\n",
        "  batch_size = 1\n",
        "  max_length = 20\n",
        "\n",
        "  test_filepath = 'data_dir/test-0.10.tsv'\n",
        "  preds_filepath = 'data_dir/pred-0.10.tsv'\n",
        "\n",
        "  with open(test_filepath, 'r', encoding='utf-8') as fin:\n",
        "    nlangs = len(next(fin).strip('\\n').split('\\t')) - 1\n",
        "\n",
        "\n",
        "  latest_ckpt_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  if not latest_ckpt_path:\n",
        "    raise ValueError('No checkpoint available')\n",
        "  logging.info('Restoring from checkpoint %s ...', latest_ckpt_path)\n",
        "  infiller = model\n",
        "  checkpoint = tf.train.Checkpoint(model)\n",
        "  checkpoint.restore(latest_ckpt_path).expect_partial()\n",
        "\n",
        "  logging.info('Generating predictions and saving results...')\n",
        "  with open(preds_filepath, 'w', encoding='utf-8') as vfile:\n",
        "    with open(test_filepath, 'r', encoding='utf-8') as tfile:\n",
        "      # Copy the header.\n",
        "      vfile.write(next(tfile))\n",
        "      for line in tfile:\n",
        "        parts = line.strip('\\n').split('\\t')\n",
        "        tset = ['<TARGET>' if p == '?' else p for p in parts[1:]]\n",
        "        tgt_index = tset.index('<TARGET>')\n",
        "        pred = silent_translate(Y_pred, tset, char2idx, max_length, idx2char)\n",
        "        row = ['' for p in parts]\n",
        "        row[0] = parts[0]\n",
        "        row[tgt_index + 1] = pred\n",
        "        vfile.write('\\t'.join(row) + '\\n')\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Training pipeline.\"\"\"\n",
        "\n",
        "    # Produce base training data and vocab, and expand the training data.\n",
        "    datadir = '/data_dir'\n",
        "    cogsets, vocab = build_base_data_and_vocab()\n",
        "    print(vocab)\n",
        "    char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "    idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "    nlangs = len(cogsets[0])\n",
        "    hparams = get_hparams()\n",
        "    vocab_size = len(vocab)\n",
        "    all_samples = expand_training_set(cogsets)\n",
        "\n",
        "    # # Read in the dev data.\n",
        "    # dev_sets = []\n",
        "    # filepath = '/content/data_dir/dev-0.10_01.tsv'\n",
        "    # with open(filepath, 'r', encoding='utf-8') as fin:\n",
        "    #   # Skip header.\n",
        "    #   next(fin)\n",
        "    #   for line in fin:\n",
        "    #     parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "    #     parts = ['<TARGET>' if p == '?' else p for p in parts]\n",
        "    #     dev_sets.append(parts)\n",
        "\n",
        "    # # Read in the dev  solution set.\n",
        "    # dev_solutions = []\n",
        "    # filepath = '/content/data_dir/dev_solutions-0.10_01.tsv'\n",
        "    # with open(filepath, 'r', encoding='utf-8') as fin:\n",
        "    #   # Skip header.\n",
        "    #   next(fin)\n",
        "    #   for line in fin:\n",
        "    #     parts = tuple(line.strip('\\n').split('\\t')[1:])\n",
        "    #     dev_solutions.append(''.join(parts).strip())\n",
        "\n",
        "    # Core settings.\n",
        "    steps_per_epoch = 500\n",
        "    batch_size = 1\n",
        "    max_length = 20\n",
        "    # Have we written the vocab and hparams already?\n",
        "    vocab_written = False\n",
        "\n",
        "    # Define the model, optimizer and loss function.\n",
        "    # infiller = Infiller(vocab_size, hparams, batch_size, nlangs, max_length)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    checkpoint_dir = '/checkpoint_dir'\n",
        "    # if checkpoint_dir:\n",
        "    #     # checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "    #     checkpoint = tf.train.Checkpoint(optimizer=optimizer, infiller=infiller)\n",
        "\n",
        "    logging.info('Training the model ...')\n",
        "    train_dataset = build_train_dataset(all_samples, batch_size, nlangs,\n",
        "                                        max_length, char2idx)\n",
        "    best_error = None\n",
        "\n",
        "    SYM_NORM = True\n",
        "    vocab_size = vocab_size\n",
        "    embedding_dim = hparams['embedding_dim']\n",
        "    embedding = np.random.randint(500, size=(nlangs, vocab_size))\n",
        "    arr = []\n",
        "    for i in range(len(embedding)):\n",
        "        c = 0\n",
        "        a = []\n",
        "        for j in range(len(embedding)):\n",
        "            cosine = np.dot(embedding[i], embedding[j]) / (norm(embedding[i]) * norm(embedding[j]))\n",
        "            a.append(cosine)\n",
        "        arr.append(a)\n",
        "    A_norm = preprocess_adj_numpy(arr, SYM_NORM)\n",
        "    num_filters = 2\n",
        "    graph_conv_filters = np.concatenate([A_norm, np.matmul(A_norm, A_norm)], axis=0)\n",
        "    graph_conv_filters = K.constant(graph_conv_filters)\n",
        "    #   print(shape(graph_conv_filters))\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    nb_epochs = 10\n",
        "    model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GraphCNN(max_length, num_filters, graph_conv_filters, activation='relu', kernel_regularizer=l2(5e-4)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
        "    for epoch in range(nb_epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for (_, (inp, targ, inp_mask,\n",
        "                 targ_mask)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "            input_i = tf.reshape(inp, [inp.shape[1], inp.shape[2]], name=None)\n",
        "            targ_i = tf.reshape(targ, [targ.shape[1], targ.shape[2]], name=None)\n",
        "            # print(shape(input_i))\n",
        "            # print(shape(targ_i))\n",
        "            input_mask = tf.reshape(inp_mask, [inp_mask.shape[1], inp_mask.shape[2]], name=None)\n",
        "            target_mask = tf.reshape(targ_mask, [targ_mask.shape[1], targ_mask.shape[2]], name=None)\n",
        "            # model.build(shape(input_i))\n",
        "            # model.summary()\n",
        "        model.fit(input_i, targ_i, batch_size=input_i.shape[0], epochs=1, shuffle=False, verbose=0)\n",
        "        Y_pred = model.predict(input_mask, batch_size=inp_mask.shape[1])\n",
        "        # print(Y_pred)\n",
        "        index = []\n",
        "        for i in range(0, len(target_mask)):\n",
        "            index.append(i)\n",
        "        loss = evaluate_preds(Y_pred, targ_mask, index)\n",
        "        # _, test_acc = evaluate_preds(Y_pred, [targ], [test_idx])\n",
        "\n",
        "        # batch_loss = train_step(infiller, optimizer, inp, inp_mask, targ,\n",
        "        #                         targ_mask)\n",
        "        print(\"Epoch: \", epoch, \"loss:\", loss)\n",
        "        \n",
        "        checkpoint = tf.train.Checkpoint( model)\n",
        "        checkpoint.save('checkpoint_dir/best_model.ckpt')\n",
        "        model1=model\n",
        "        # print(\"Epoch: {:04d}\".format(epoch), \"train_acc= {:.4f}\".format(train_loss))\n",
        "        # print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "        # total_loss / steps_per_epoch))\n",
        "\n",
        "        # print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "        # Evaluate on dev set:\n",
        "        # derrors = [0 for l in range(nlangs)]\n",
        "        # dtotals = [0 for l in range(nlangs)]\n",
        "        # allerrors = 0\n",
        "        # for dset, dsol in zip(dev_sets, dev_solutions):\n",
        "        #   tgt_index = dset.index('<TARGET>')\n",
        "        #   dtotals[tgt_index] += 1\n",
        "        #   pred = silent_translate(infiller, dset, char2idx, max_length, idx2char)\n",
        "        #   if pred != dsol:\n",
        "        #     derrors[tgt_index] += 1\n",
        "        #     allerrors += 1\n",
        "        # derrors = [x / y for x, y in zip(derrors, dtotals) if y != 0]\n",
        "        # mean_accuracy = np.mean(derrors)\n",
        "\n",
        "        # Update based on dev set.\n",
        "        # if not best_error or mean_accuracy <= best_error:\n",
        "        #   print('ERROR_UPDATE:', derrors)\n",
        "        # if checkpoint_dir:\n",
        "        #   checkpoint.save('/checkpoint_dir/best_model.ckpt')\n",
        "        #     # Write the vocab AFTER ensuring checkpoint dir has been created.\n",
        "        if not vocab_written:\n",
        "            # Write the model parameters.\n",
        "            hparams = get_hparams()\n",
        "            hparams['embedding_dim'] = hparams[\"embedding_dim\"]\n",
        "            hparams['kernel_width'] = hparams[\"kernel_width\"]\n",
        "            hparams['filters'] = hparams[\"filters\"]\n",
        "            hparams['dropout'] = hparams[\"dropout\"]\n",
        "            hparams['nonlinearity'] = hparams[\"nonlinearity\"]\n",
        "            hparams['sfactor'] = hparams[\"sfactor\"]\n",
        "            with open('checkpoint_dir/' + 'hparams.json', 'w') as vfile:\n",
        "                json.dump(hparams, vfile)\n",
        "            #       # Write the vocabulary.\n",
        "            with open('checkpoint_dir/' + 'vocab.txt', 'w', encoding='utf-8') as vfile:\n",
        "                for v in vocab:\n",
        "                    vfile.write(v + '\\n')\n",
        "        #       vocab_written = True\n",
        "        #   best_error = mean_accuracy\n",
        "        # print(best_error, mean_accuracy, '\\n')\n",
        "    decode_with_model(Y_pred,model)\n",
        "    # For some reason this step takes a couple of minutes to complete using\n",
        "    # Tensorflow 2.8.0.\n",
        "    logging.info('Done. Shutting down ...')\n",
        "\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ilsGIsTnaDg"
      },
      "outputs": [],
      "source": [
        "def get_vocab(checkpoint_dir):\n",
        "  file_path =\"checkpoint_dir/vocab.txt\"\n",
        "  if not file_path:\n",
        "    raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "  logging.info('Loading vocab from %s ...', file_path)\n",
        "  with open(file_path, 'r', encoding='utf8') as f:\n",
        "    vocab = [symbol.strip() for symbol in f if symbol]\n",
        "  logging.info('%d symbols loaded.', len(vocab))\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "093fZ15zcRXM"
      },
      "outputs": [],
      "source": [
        "def decode_with_model(model):\n",
        "  hparams = get_hparams()\n",
        "  checkpoint_dir='checkpoint_dir/'\n",
        "  vocab = get_vocab(\"checkpoint_dir/\")\n",
        "  char2idx = {vocab[i]: i for i in range(len(vocab))}\n",
        "  idx2char = {i: vocab[i] for i in range(len(vocab))}\n",
        "  vocab_size = len(vocab)\n",
        "  batch_size = 1\n",
        "  max_length = 20\n",
        "\n",
        "  test_filepath = 'data_dir/test-0.10.tsv'\n",
        "  preds_filepath = 'data_dir/pred-0.10.tsv'\n",
        "\n",
        "  with open(test_filepath, 'r', encoding='utf-8') as fin:\n",
        "    nlangs = len(next(fin).strip('\\n').split('\\t')) - 1\n",
        "\n",
        "\n",
        "  latest_ckpt_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  if not latest_ckpt_path:\n",
        "    raise ValueError('No checkpoint available')\n",
        "  logging.info('Restoring from checkpoint %s ...', latest_ckpt_path)\n",
        "  infiller = model\n",
        "  checkpoint = tf.train.Checkpoint(model)\n",
        "  checkpoint.restore(latest_ckpt_path).expect_partial()\n",
        "\n",
        "  logging.info('Generating predictions and saving results...')\n",
        "  with open(preds_filepath, 'w', encoding='utf-8') as vfile:\n",
        "    with open(test_filepath, 'r', encoding='utf-8') as tfile:\n",
        "      # Copy the header.\n",
        "      vfile.write(next(tfile))\n",
        "      for line in tfile:\n",
        "        parts = line.strip('\\n').split('\\t')\n",
        "        tset = ['<TARGET>' if p == '?' else p for p in parts[1:]]\n",
        "        tgt_index = tset.index('<TARGET>')\n",
        "        pred = silent_translate(infiller, tset, char2idx, max_length, idx2char)\n",
        "        row = ['' for p in parts]\n",
        "        row[0] = parts[0]\n",
        "        row[tgt_index + 1] = pred\n",
        "        vfile.write('\\t'.join(row) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8j_UY07tvyn",
        "outputId": "491e7b49-28f4-411b-9bcf-3a6b7b05b721"
      },
      "outputs": [],
      "source": [
        "%pip install lingrex\n",
        "%pip install lingpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFQTxwu8_83x",
        "outputId": "fba22c2a-b776-4475-e1ea-a307e6e3f98e"
      },
      "outputs": [],
      "source": [
        "from lingrex.util import bleu_score\n",
        "from lingpy import *\n",
        "from lingpy.evaluate.acd import _get_bcubed_score as bcubed_score\n",
        "from tabulate import tabulate\n",
        "from collections import defaultdict\n",
        "from lingpy.sequence.ngrams import get_n_ngrams\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2dTmkLgPtzmL"
      },
      "outputs": [],
      "source": [
        "def load_cognate_file(path):\n",
        "    \"\"\"\n",
        "    Helper function for simplified cognate formats.\n",
        "    \"\"\"\n",
        "    data = csv2list(path, strip_lines=False)\n",
        "    header = data[0]\n",
        "    languages = header[1:]\n",
        "    out = {}\n",
        "    sounds = defaultdict(lambda : defaultdict(list))\n",
        "    for row in data[1:]:\n",
        "        out[row[0]] = {}\n",
        "        for language, entry in zip(languages, row[1:]):\n",
        "            out[row[0]][language] = entry.split()\n",
        "            for i, sound in enumerate(entry.split()):\n",
        "                sounds[sound][language] += [[row[0], i]]\n",
        "    # print(\"Languages:::::\",languages)\n",
        "    # print(\"Sound::::\",sounds)\n",
        "    # print(\"OUT::::\",out)\n",
        "    return languages,sounds, out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TwVKRhbjt3Um"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def bleu_score(word, reference, n=4, weights=None, trim=False):\n",
        "    \"\"\"\n",
        "    Compute the BLEU score for predicted word and reference.\n",
        "    \"\"\"\n",
        "\n",
        "    if not weights:\n",
        "        weights = [1 / n for x in range(n)]\n",
        "\n",
        "    scores = []\n",
        "    for i in range(1, n + 1):\n",
        "\n",
        "        new_wrd = list(get_n_ngrams(word, i))\n",
        "        new_ref = list(get_n_ngrams(reference, i))\n",
        "        if trim and i > 1:\n",
        "            new_wrd = new_wrd[i - 1 : -(i - 1)]\n",
        "            new_ref = new_ref[i - 1 : -(i - 1)]\n",
        "\n",
        "        clipped, divide = [], []\n",
        "        for itm in set(new_wrd):\n",
        "            clipped += [new_ref.count(itm)]\n",
        "            divide += [new_wrd.count(itm)]\n",
        "        scores += [sum(clipped) / sum(divide)]\n",
        "\n",
        "    # calculate arithmetic mean\n",
        "    out_score = 1\n",
        "    for weight, score in zip(weights, scores):\n",
        "        out_score = out_score * (score**weight)\n",
        "\n",
        "    if len(word) > len(reference):\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.e ** (1 - (len(reference) / len(word)))\n",
        "    return bp * (out_score ** (1 / sum(weights)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9h8nLazmAKig"
      },
      "outputs": [],
      "source": [
        "def compare_words(firstfile, secondfile, report=True):\n",
        "    \"\"\"\n",
        "    Evaluate the predicted and attested words in two datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    (languages, soundsA, first), (languagesB, soundsB, last) = load_cognate_file(firstfile), load_cognate_file(secondfile)\n",
        "    print(\"///\",languages, soundsA, first)\n",
        "    all_scores = []\n",
        "    for language in languages:\n",
        "        scores = []\n",
        "        almsA, almsB = [], []\n",
        "        for key in first:\n",
        "            if language in first[key]:\n",
        "                entryA = first[key][language]\n",
        "                # print(\"@@@@\",entryA)\n",
        "                if \" \".join(entryA):\n",
        "                    try:\n",
        "                        # print(\"&&&&\",entryA)\n",
        "                        entryB = last[key][language]\n",
        "                        # print(\"####\",entryB)\n",
        "                    except KeyError:\n",
        "                        print(\"Missing entry {0} / {1} / {2}\".format(\n",
        "                            key, language, secondfile))\n",
        "                        entryB = \"\"\n",
        "                    if not entryB:\n",
        "                        entryB = (2 * len(entryA)) * [\"Ø\"]\n",
        "                    # print(entryA)\n",
        "                    # print(entryB)\n",
        "                    almA, almB, _ = nw_align(entryA, entryB)\n",
        "                    almsA += almA\n",
        "                    almsB += almB\n",
        "                    score = 0\n",
        "                    for a, b in zip(almA, almB):\n",
        "                        if a == b and a not in \"Ø?-\":\n",
        "                            pass\n",
        "                        elif a != b:\n",
        "                            score += 1\n",
        "                    scoreD = score / len(almA)\n",
        "                    bleu = bleu_score(entryA, entryB, n=4, trim=False)\n",
        "                    scores += [[key, entryA, entryB, score, scoreD, bleu]]\n",
        "        if scores:\n",
        "            p, r = bcubed_score(almsA, almsB), bcubed_score(almsB, almsA)\n",
        "            fs = 2 * (p*r) / (p+r)\n",
        "            all_scores += [[\n",
        "                language,\n",
        "                sum([row[-3] for row in scores])/len(scores),\n",
        "                sum([row[-2] for row in scores])/len(scores),\n",
        "                fs,\n",
        "                sum([row[-1] for row in scores])/len(scores)]]\n",
        "    all_scores += [[\n",
        "        \"TOTAL\", \n",
        "        sum([row[-4] for row in all_scores])/len(languages),\n",
        "        sum([row[-3] for row in all_scores])/len(languages),\n",
        "        sum([row[-2] for row in all_scores])/len(languages),\n",
        "        sum([row[-1] for row in all_scores])/len(languages),\n",
        "        ]]\n",
        "    if report:\n",
        "        print(\n",
        "                tabulate(\n",
        "                    all_scores, \n",
        "                    headers=[\n",
        "                        \"Language\", \"ED\", \"ED (Normalized)\", \n",
        "                        \"B-Cubed FS\", \"BLEU\"], floatfmt=\".3f\"))\n",
        "    return all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MZReSik5AQAg"
      },
      "outputs": [],
      "source": [
        "# compare_words('/content/data_dir/pred-0.10.tsv','/content/data_dir/solutions-0.10.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40B-pZRQbqVW",
        "outputId": "d5d0e6c6-b1e7-440b-e996-3898312a51d2"
      },
      "outputs": [],
      "source": [
        "compare_words('/content/data_dir/pred-0.10.tsv','/content/data_dir/solutions-0.10.tsv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "c8376c60763d0532fb7a4c462f5427e7424e26d41c55767d9d476c0454c24711"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
